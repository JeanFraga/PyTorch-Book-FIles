{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "565e01a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.334072  [    0/60000]\n",
      "loss: 0.637571  [ 6400/60000]\n",
      "loss: 0.474858  [12800/60000]\n",
      "loss: 0.441892  [19200/60000]\n",
      "loss: 0.619886  [25600/60000]\n",
      "loss: 0.309498  [32000/60000]\n",
      "loss: 0.352975  [38400/60000]\n",
      "loss: 0.504916  [44800/60000]\n",
      "loss: 0.618955  [51200/60000]\n",
      "loss: 0.515899  [57600/60000]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.445866  [    0/60000]\n",
      "loss: 0.495442  [ 6400/60000]\n",
      "loss: 0.503632  [12800/60000]\n",
      "loss: 0.452827  [19200/60000]\n",
      "loss: 0.274984  [25600/60000]\n",
      "loss: 0.444188  [32000/60000]\n",
      "loss: 0.203212  [38400/60000]\n",
      "loss: 0.267432  [44800/60000]\n",
      "loss: 0.444034  [51200/60000]\n",
      "loss: 0.430673  [57600/60000]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.192283  [    0/60000]\n",
      "loss: 0.588697  [ 6400/60000]\n",
      "loss: 0.181197  [12800/60000]\n",
      "loss: 0.299185  [19200/60000]\n",
      "loss: 0.280489  [25600/60000]\n",
      "loss: 0.202072  [32000/60000]\n",
      "loss: 0.320793  [38400/60000]\n",
      "loss: 0.507163  [44800/60000]\n",
      "loss: 0.450777  [51200/60000]\n",
      "loss: 0.324720  [57600/60000]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.442088  [    0/60000]\n",
      "loss: 0.322661  [ 6400/60000]\n",
      "loss: 0.239301  [12800/60000]\n",
      "loss: 0.385492  [19200/60000]\n",
      "loss: 0.270263  [25600/60000]\n",
      "loss: 0.158726  [32000/60000]\n",
      "loss: 0.235435  [38400/60000]\n",
      "loss: 0.411530  [44800/60000]\n",
      "loss: 0.313714  [51200/60000]\n",
      "loss: 0.184379  [57600/60000]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.508220  [    0/60000]\n",
      "loss: 0.296450  [ 6400/60000]\n",
      "loss: 0.295712  [12800/60000]\n",
      "loss: 0.356175  [19200/60000]\n",
      "loss: 0.392219  [25600/60000]\n",
      "loss: 0.202316  [32000/60000]\n",
      "loss: 0.481712  [38400/60000]\n",
      "loss: 0.343371  [44800/60000]\n",
      "loss: 0.275537  [51200/60000]\n",
      "loss: 0.254550  [57600/60000]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Define the model\n",
    "class FashionMNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionMNISTModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),  # 10 classes for FashionMNIST\n",
    "            nn.LogSoftmax(dim=1)  # LogSoftmax for multi-class classification\n",
    "        ) # often abbreviated to \"ops\"\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits # log probabilities that indicate the confidence of the model\n",
    "\n",
    "model = FashionMNISTModel()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.NLLLoss() # Negative Log Likelihood Loss - for multi-class classification\n",
    "# passes parameters to the optimizer to optimize for lower loss\n",
    "optimizer = optim.Adam(model.parameters()) # Faster and more accurate than SGD\n",
    "\n",
    "# Train the model\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # model can be set to different modes to optimize performance for training, inference or evaluation\n",
    "    model.train() # Set the model to training mode, which enables dropout and batch normalization layers\n",
    "    for batch, (X,y) in enumerate(dataloader): # Use dataloader only loads the data when needed\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \n",
    "# Training process\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_function, optimizer)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a261062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.368818 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to test the model\n",
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() # set the model to evaluation mode\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad(): # no need to compute gradients during evaluation\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_function(pred, y).item()\n",
    "            correct += (pred.argmax(1) ==\n",
    "                        y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "# Evaluate the model\n",
    "test(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb013ff",
   "metadata": {},
   "source": [
    "## Explore the Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40e3944e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-12.5197, -16.5524, -14.6458, -13.8267, -10.7658,  -2.2793, -10.9626,\n",
      "          -2.0535,  -8.3655,  -0.2625]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicted 9, and the actual label is 9.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict_single_image(image, label, model):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Unsqueeze image as the model expects a batch dimension\n",
    "    image = image.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(image)\n",
    "        print(prediction)\n",
    "        predicted_label = prediction.argmax(1).item()\n",
    "\n",
    "    # Display the image and predictions\n",
    "    plt.imshow(image.squeeze(), cmap='gray')\n",
    "    plt.title(f'Predicted: {predicted_label}, Actual: {label}')\n",
    "    plt.show()\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "# Choose an image from the test set\n",
    "image, label = test_dataset[0] # Change index to test different images\n",
    "\n",
    "# Predict the class for the chosen image\n",
    "predicted_label = predict_single_image(image, label, model)\n",
    "print(f\"The model predicted {predicted_label}, and the actual label is {label}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec1d47",
   "metadata": {},
   "source": [
    "## Early Stopping Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944da77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch 0, Loss: 2.298838, Accuracy: 10.94% [    0/60000]\n",
      "Batch 100, Loss: 0.960211, Accuracy: 68.70% [ 6400/60000]\n",
      "Batch 200, Loss: 0.778762, Accuracy: 74.48% [12800/60000]\n",
      "Batch 300, Loss: 0.700082, Accuracy: 76.51% [19200/60000]\n",
      "Batch 400, Loss: 0.649925, Accuracy: 78.09% [25600/60000]\n",
      "Batch 500, Loss: 0.614288, Accuracy: 79.20% [32000/60000]\n",
      "Batch 600, Loss: 0.590900, Accuracy: 79.95% [38400/60000]\n",
      "Batch 700, Loss: 0.573772, Accuracy: 80.49% [44800/60000]\n",
      "Batch 800, Loss: 0.560105, Accuracy: 80.86% [51200/60000]\n",
      "Batch 900, Loss: 0.547899, Accuracy: 81.21% [57600/60000]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.423711, Accuracy: 84.38% [    0/60000]\n",
      "Batch 100, Loss: 0.425381, Accuracy: 84.82% [ 6400/60000]\n",
      "Batch 200, Loss: 0.421625, Accuracy: 85.08% [12800/60000]\n",
      "Batch 300, Loss: 0.422464, Accuracy: 85.14% [19200/60000]\n",
      "Batch 400, Loss: 0.417105, Accuracy: 85.25% [25600/60000]\n",
      "Batch 500, Loss: 0.414943, Accuracy: 85.35% [32000/60000]\n",
      "Batch 600, Loss: 0.409309, Accuracy: 85.48% [38400/60000]\n",
      "Batch 700, Loss: 0.406451, Accuracy: 85.53% [44800/60000]\n",
      "Batch 800, Loss: 0.405167, Accuracy: 85.58% [51200/60000]\n",
      "Batch 900, Loss: 0.402298, Accuracy: 85.70% [57600/60000]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.324658, Accuracy: 85.94% [    0/60000]\n",
      "Batch 100, Loss: 0.351268, Accuracy: 87.59% [ 6400/60000]\n",
      "Batch 200, Loss: 0.361582, Accuracy: 87.15% [12800/60000]\n",
      "Batch 300, Loss: 0.362418, Accuracy: 87.03% [19200/60000]\n",
      "Batch 400, Loss: 0.363462, Accuracy: 86.90% [25600/60000]\n",
      "Batch 500, Loss: 0.364630, Accuracy: 86.89% [32000/60000]\n",
      "Batch 600, Loss: 0.362662, Accuracy: 86.96% [38400/60000]\n",
      "Batch 700, Loss: 0.359904, Accuracy: 87.05% [44800/60000]\n",
      "Batch 800, Loss: 0.358053, Accuracy: 87.12% [51200/60000]\n",
      "Batch 900, Loss: 0.357521, Accuracy: 87.17% [57600/60000]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.235473, Accuracy: 90.62% [    0/60000]\n",
      "Batch 100, Loss: 0.328838, Accuracy: 87.87% [ 6400/60000]\n",
      "Batch 200, Loss: 0.336993, Accuracy: 87.86% [12800/60000]\n",
      "Batch 300, Loss: 0.332573, Accuracy: 88.04% [19200/60000]\n",
      "Batch 400, Loss: 0.333769, Accuracy: 88.03% [25600/60000]\n",
      "Batch 500, Loss: 0.331966, Accuracy: 88.06% [32000/60000]\n",
      "Batch 600, Loss: 0.328705, Accuracy: 88.09% [38400/60000]\n",
      "Batch 700, Loss: 0.329189, Accuracy: 88.08% [44800/60000]\n",
      "Batch 800, Loss: 0.328542, Accuracy: 88.13% [51200/60000]\n",
      "Batch 900, Loss: 0.330225, Accuracy: 88.07% [57600/60000]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.346033, Accuracy: 85.94% [    0/60000]\n",
      "Batch 100, Loss: 0.315356, Accuracy: 88.41% [ 6400/60000]\n",
      "Batch 200, Loss: 0.313781, Accuracy: 88.57% [12800/60000]\n",
      "Batch 300, Loss: 0.318345, Accuracy: 88.33% [19200/60000]\n",
      "Batch 400, Loss: 0.318155, Accuracy: 88.41% [25600/60000]\n",
      "Batch 500, Loss: 0.318837, Accuracy: 88.47% [32000/60000]\n",
      "Batch 600, Loss: 0.315442, Accuracy: 88.64% [38400/60000]\n",
      "Batch 700, Loss: 0.313160, Accuracy: 88.66% [44800/60000]\n",
      "Batch 800, Loss: 0.313222, Accuracy: 88.69% [51200/60000]\n",
      "Batch 900, Loss: 0.313606, Accuracy: 88.67% [57600/60000]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.222934, Accuracy: 92.19% [    0/60000]\n",
      "Batch 100, Loss: 0.286062, Accuracy: 89.51% [ 6400/60000]\n",
      "Batch 200, Loss: 0.288402, Accuracy: 89.44% [12800/60000]\n",
      "Batch 300, Loss: 0.291997, Accuracy: 89.28% [19200/60000]\n",
      "Batch 400, Loss: 0.293884, Accuracy: 89.14% [25600/60000]\n",
      "Batch 500, Loss: 0.293256, Accuracy: 89.30% [32000/60000]\n",
      "Batch 600, Loss: 0.296384, Accuracy: 89.15% [38400/60000]\n",
      "Batch 700, Loss: 0.297163, Accuracy: 89.12% [44800/60000]\n",
      "Batch 800, Loss: 0.296696, Accuracy: 89.09% [51200/60000]\n",
      "Batch 900, Loss: 0.293692, Accuracy: 89.17% [57600/60000]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.225694, Accuracy: 90.62% [    0/60000]\n",
      "Batch 100, Loss: 0.272995, Accuracy: 90.05% [ 6400/60000]\n",
      "Batch 200, Loss: 0.279782, Accuracy: 89.75% [12800/60000]\n",
      "Batch 300, Loss: 0.280609, Accuracy: 89.63% [19200/60000]\n",
      "Batch 400, Loss: 0.283037, Accuracy: 89.57% [25600/60000]\n",
      "Batch 500, Loss: 0.280535, Accuracy: 89.70% [32000/60000]\n",
      "Batch 600, Loss: 0.279177, Accuracy: 89.76% [38400/60000]\n",
      "Batch 700, Loss: 0.281093, Accuracy: 89.69% [44800/60000]\n",
      "Batch 800, Loss: 0.281530, Accuracy: 89.67% [51200/60000]\n",
      "Batch 900, Loss: 0.280301, Accuracy: 89.71% [57600/60000]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.226678, Accuracy: 93.75% [    0/60000]\n",
      "Batch 100, Loss: 0.271814, Accuracy: 90.36% [ 6400/60000]\n",
      "Batch 200, Loss: 0.268546, Accuracy: 90.20% [12800/60000]\n",
      "Batch 300, Loss: 0.271799, Accuracy: 90.05% [19200/60000]\n",
      "Batch 400, Loss: 0.271105, Accuracy: 90.10% [25600/60000]\n",
      "Batch 500, Loss: 0.272572, Accuracy: 90.12% [32000/60000]\n",
      "Batch 600, Loss: 0.271891, Accuracy: 90.13% [38400/60000]\n",
      "Batch 700, Loss: 0.271795, Accuracy: 90.04% [44800/60000]\n",
      "Batch 800, Loss: 0.272517, Accuracy: 90.02% [51200/60000]\n",
      "Batch 900, Loss: 0.272111, Accuracy: 90.01% [57600/60000]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.453116, Accuracy: 84.38% [    0/60000]\n",
      "Batch 100, Loss: 0.265608, Accuracy: 90.32% [ 6400/60000]\n",
      "Batch 200, Loss: 0.262700, Accuracy: 90.40% [12800/60000]\n",
      "Batch 300, Loss: 0.261360, Accuracy: 90.51% [19200/60000]\n",
      "Batch 400, Loss: 0.265559, Accuracy: 90.35% [25600/60000]\n",
      "Batch 500, Loss: 0.263881, Accuracy: 90.35% [32000/60000]\n",
      "Batch 600, Loss: 0.262337, Accuracy: 90.37% [38400/60000]\n",
      "Batch 700, Loss: 0.260968, Accuracy: 90.38% [44800/60000]\n",
      "Batch 800, Loss: 0.260917, Accuracy: 90.38% [51200/60000]\n",
      "Batch 900, Loss: 0.261390, Accuracy: 90.32% [57600/60000]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.395184, Accuracy: 85.94% [    0/60000]\n",
      "Batch 100, Loss: 0.257309, Accuracy: 90.45% [ 6400/60000]\n",
      "Batch 200, Loss: 0.251309, Accuracy: 90.63% [12800/60000]\n",
      "Batch 300, Loss: 0.249353, Accuracy: 90.69% [19200/60000]\n",
      "Batch 400, Loss: 0.248010, Accuracy: 90.74% [25600/60000]\n",
      "Batch 500, Loss: 0.251488, Accuracy: 90.63% [32000/60000]\n",
      "Batch 600, Loss: 0.252398, Accuracy: 90.54% [38400/60000]\n",
      "Batch 700, Loss: 0.251332, Accuracy: 90.67% [44800/60000]\n",
      "Batch 800, Loss: 0.252515, Accuracy: 90.64% [51200/60000]\n",
      "Batch 900, Loss: 0.252165, Accuracy: 90.68% [57600/60000]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.132519, Accuracy: 95.31% [    0/60000]\n",
      "Batch 100, Loss: 0.238748, Accuracy: 90.95% [ 6400/60000]\n",
      "Batch 200, Loss: 0.235892, Accuracy: 91.19% [12800/60000]\n",
      "Batch 300, Loss: 0.239199, Accuracy: 91.08% [19200/60000]\n",
      "Batch 400, Loss: 0.240582, Accuracy: 91.03% [25600/60000]\n",
      "Batch 500, Loss: 0.238979, Accuracy: 91.12% [32000/60000]\n",
      "Batch 600, Loss: 0.239616, Accuracy: 91.13% [38400/60000]\n",
      "Batch 700, Loss: 0.241748, Accuracy: 91.06% [44800/60000]\n",
      "Batch 800, Loss: 0.243085, Accuracy: 91.02% [51200/60000]\n",
      "Batch 900, Loss: 0.244108, Accuracy: 91.00% [57600/60000]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.162161, Accuracy: 93.75% [    0/60000]\n",
      "Batch 100, Loss: 0.244800, Accuracy: 90.83% [ 6400/60000]\n",
      "Batch 200, Loss: 0.237030, Accuracy: 91.07% [12800/60000]\n",
      "Batch 300, Loss: 0.238373, Accuracy: 91.15% [19200/60000]\n",
      "Batch 400, Loss: 0.234813, Accuracy: 91.20% [25600/60000]\n",
      "Batch 500, Loss: 0.235591, Accuracy: 91.26% [32000/60000]\n",
      "Batch 600, Loss: 0.236460, Accuracy: 91.21% [38400/60000]\n",
      "Batch 700, Loss: 0.236908, Accuracy: 91.25% [44800/60000]\n",
      "Batch 800, Loss: 0.236762, Accuracy: 91.22% [51200/60000]\n",
      "Batch 900, Loss: 0.236423, Accuracy: 91.27% [57600/60000]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.172349, Accuracy: 96.88% [    0/60000]\n",
      "Batch 100, Loss: 0.220050, Accuracy: 91.96% [ 6400/60000]\n",
      "Batch 200, Loss: 0.224649, Accuracy: 91.58% [12800/60000]\n",
      "Batch 300, Loss: 0.227760, Accuracy: 91.38% [19200/60000]\n",
      "Batch 400, Loss: 0.224596, Accuracy: 91.59% [25600/60000]\n",
      "Batch 500, Loss: 0.225098, Accuracy: 91.60% [32000/60000]\n",
      "Batch 600, Loss: 0.224597, Accuracy: 91.66% [38400/60000]\n",
      "Batch 700, Loss: 0.225402, Accuracy: 91.56% [44800/60000]\n",
      "Batch 800, Loss: 0.225819, Accuracy: 91.59% [51200/60000]\n",
      "Batch 900, Loss: 0.228182, Accuracy: 91.49% [57600/60000]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.168096, Accuracy: 93.75% [    0/60000]\n",
      "Batch 100, Loss: 0.210570, Accuracy: 92.11% [ 6400/60000]\n",
      "Batch 200, Loss: 0.212348, Accuracy: 92.00% [12800/60000]\n",
      "Batch 300, Loss: 0.210542, Accuracy: 92.11% [19200/60000]\n",
      "Batch 400, Loss: 0.214740, Accuracy: 91.91% [25600/60000]\n",
      "Batch 500, Loss: 0.216570, Accuracy: 91.92% [32000/60000]\n",
      "Batch 600, Loss: 0.217604, Accuracy: 91.88% [38400/60000]\n",
      "Batch 700, Loss: 0.219210, Accuracy: 91.82% [44800/60000]\n",
      "Batch 800, Loss: 0.221948, Accuracy: 91.73% [51200/60000]\n",
      "Batch 900, Loss: 0.222345, Accuracy: 91.75% [57600/60000]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.228960, Accuracy: 92.19% [    0/60000]\n",
      "Batch 100, Loss: 0.217909, Accuracy: 92.03% [ 6400/60000]\n",
      "Batch 200, Loss: 0.223383, Accuracy: 91.80% [12800/60000]\n",
      "Batch 300, Loss: 0.219795, Accuracy: 91.98% [19200/60000]\n",
      "Batch 400, Loss: 0.216690, Accuracy: 92.03% [25600/60000]\n",
      "Batch 500, Loss: 0.218235, Accuracy: 91.89% [32000/60000]\n",
      "Batch 600, Loss: 0.218067, Accuracy: 91.88% [38400/60000]\n",
      "Batch 700, Loss: 0.218277, Accuracy: 91.90% [44800/60000]\n",
      "Batch 800, Loss: 0.217959, Accuracy: 91.94% [51200/60000]\n",
      "Batch 900, Loss: 0.216366, Accuracy: 92.00% [57600/60000]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.224635, Accuracy: 90.62% [    0/60000]\n",
      "Batch 100, Loss: 0.207453, Accuracy: 92.03% [ 6400/60000]\n",
      "Batch 200, Loss: 0.213232, Accuracy: 92.04% [12800/60000]\n",
      "Batch 300, Loss: 0.211069, Accuracy: 92.15% [19200/60000]\n",
      "Batch 400, Loss: 0.210798, Accuracy: 92.11% [25600/60000]\n",
      "Batch 500, Loss: 0.213638, Accuracy: 92.06% [32000/60000]\n",
      "Batch 600, Loss: 0.212581, Accuracy: 92.06% [38400/60000]\n",
      "Batch 700, Loss: 0.211730, Accuracy: 92.05% [44800/60000]\n",
      "Batch 800, Loss: 0.211530, Accuracy: 92.07% [51200/60000]\n",
      "Batch 900, Loss: 0.211685, Accuracy: 92.05% [57600/60000]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.109944, Accuracy: 95.31% [    0/60000]\n",
      "Batch 100, Loss: 0.189250, Accuracy: 92.90% [ 6400/60000]\n",
      "Batch 200, Loss: 0.194934, Accuracy: 92.68% [12800/60000]\n",
      "Batch 300, Loss: 0.198728, Accuracy: 92.70% [19200/60000]\n",
      "Batch 400, Loss: 0.202621, Accuracy: 92.51% [25600/60000]\n",
      "Batch 500, Loss: 0.207376, Accuracy: 92.32% [32000/60000]\n",
      "Batch 600, Loss: 0.205962, Accuracy: 92.36% [38400/60000]\n",
      "Batch 700, Loss: 0.205785, Accuracy: 92.34% [44800/60000]\n",
      "Batch 800, Loss: 0.205605, Accuracy: 92.33% [51200/60000]\n",
      "Batch 900, Loss: 0.204799, Accuracy: 92.41% [57600/60000]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.113959, Accuracy: 96.88% [    0/60000]\n",
      "Batch 100, Loss: 0.207107, Accuracy: 92.51% [ 6400/60000]\n",
      "Batch 200, Loss: 0.203721, Accuracy: 92.58% [12800/60000]\n",
      "Batch 300, Loss: 0.201949, Accuracy: 92.59% [19200/60000]\n",
      "Batch 400, Loss: 0.200724, Accuracy: 92.67% [25600/60000]\n",
      "Batch 500, Loss: 0.202932, Accuracy: 92.52% [32000/60000]\n",
      "Batch 600, Loss: 0.200120, Accuracy: 92.63% [38400/60000]\n",
      "Batch 700, Loss: 0.200278, Accuracy: 92.62% [44800/60000]\n",
      "Batch 800, Loss: 0.199988, Accuracy: 92.65% [51200/60000]\n",
      "Batch 900, Loss: 0.199314, Accuracy: 92.66% [57600/60000]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.093786, Accuracy: 98.44% [    0/60000]\n",
      "Reached 95% accuracy, stopping training\n",
      "Early stopping triggered.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Define the model\n",
    "class FashionMNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionMNISTModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),  # 10 classes for FashionMNIST\n",
    "            nn.LogSoftmax(dim=1)  # LogSoftmax for multi-class classification\n",
    "        ) # often abbreviated to \"ops\"\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits # log probabilities that indicate the confidence of the model\n",
    "\n",
    "model = FashionMNISTModel()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.NLLLoss() # Negative Log Likelihood Loss - for multi-class classification\n",
    "# passes parameters to the optimizer to optimize for lower loss\n",
    "optimizer = optim.Adam(model.parameters()) # Faster and more accurate than SGD\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def get_accuracy(pred, labels):\n",
    "    _, predictions = torch.max(pred, 1)  # Get the index of the max log-probability\n",
    "    correct = (predictions == labels).float().sum() # Count correct predictions\n",
    "    accuracy = correct / labels.shape[0]  # Normalize by the number of labels\n",
    "    return accuracy\n",
    "\n",
    "# Train the model\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    # model can be set to different modes to optimize performance for training, inference or evaluation\n",
    "    model.train() # Set the model to training mode, which enables dropout and batch normalization layers\n",
    "    for batch, (X,y) in enumerate(dataloader): # Use dataloader only loads the data when needed\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        accuracy = get_accuracy(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()  # Store the loss for logging\n",
    "        total_accuracy += accuracy.item()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            current = batch * len(X)\n",
    "            avg_loss = total_loss / (batch +1)\n",
    "            avg_accuracy = total_accuracy / (batch + 1) * 100\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"Batch {batch}, Loss: {avg_loss:>7f}, Accuracy: {avg_accuracy:>0.2f}% [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "        if avg_accuracy >= 95:\n",
    "            print(\"Reached 95% accuracy, stopping training\")\n",
    "            return True # Stop training\n",
    "\n",
    "# Training process\n",
    "epochs = 50\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    if train(train_loader, model, loss_function, optimizer):\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b85b197b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.350219 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() # set the model to evaluation mode\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad(): # no need to compute gradients during evaluation\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_function(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "# Evaluate the model\n",
    "test(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5ca897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
